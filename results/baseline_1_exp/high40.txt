Error: [Errno 13] Permission denied: '/home/users/nus/e1506251/.venv/bin/activate.csh'
/var/spool/pbs/mom_priv/jobs/9742479.pbs101.SC: line 13: myenv/bin/activate: No such file or directory
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: timm==0.6.7 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.6.7)
Requirement already satisfied: pillow==9.2.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (9.2.0)
Requirement already satisfied: matplotlib==3.5.3 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.5.3)
Requirement already satisfied: torchprofile==0.0.4 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.0.4)
Requirement already satisfied: seaborn in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.13.2)
Requirement already satisfied: scikit-learn in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (1.6.1)
Requirement already satisfied: numpy==1.26.4 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.26.4)
Requirement already satisfied: torch>=1.4 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from timm==0.6.7->-r requirements.txt (line 1)) (2.6.0)
Requirement already satisfied: torchvision in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from timm==0.6.7->-r requirements.txt (line 1)) (0.21.0)
Requirement already satisfied: cycler>=0.10 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from matplotlib==3.5.3->-r requirements.txt (line 3)) (0.12.1)
Requirement already satisfied: fonttools>=4.22.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from matplotlib==3.5.3->-r requirements.txt (line 3)) (4.56.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from matplotlib==3.5.3->-r requirements.txt (line 3)) (1.4.8)
Requirement already satisfied: packaging>=20.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from matplotlib==3.5.3->-r requirements.txt (line 3)) (24.2)
Requirement already satisfied: pyparsing>=2.2.1 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from matplotlib==3.5.3->-r requirements.txt (line 3)) (3.2.1)
Requirement already satisfied: python-dateutil>=2.7 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from matplotlib==3.5.3->-r requirements.txt (line 3)) (2.9.0.post0)
Requirement already satisfied: pandas>=1.2 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from seaborn->-r requirements.txt (line 5)) (2.2.3)
Requirement already satisfied: scipy>=1.6.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.15.2)
Requirement already satisfied: joblib>=1.2.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 6)) (3.5.0)
Requirement already satisfied: pytz>=2020.1 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 5)) (2025.1)
Requirement already satisfied: tzdata>=2022.7 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from pandas>=1.2->seaborn->-r requirements.txt (line 5)) (2025.1)
Requirement already satisfied: six>=1.5 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib==3.5.3->-r requirements.txt (line 3)) (1.17.0)
Requirement already satisfied: filelock in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (3.17.0)
Requirement already satisfied: typing-extensions>=4.10.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (4.12.2)
Requirement already satisfied: networkx in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (3.4.2)
Requirement already satisfied: jinja2 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (3.1.6)
Requirement already satisfied: fsspec in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (2025.3.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (12.4.127)
Requirement already satisfied: triton==3.2.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (3.2.0)
Requirement already satisfied: sympy==1.13.1 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/users/nus/e1506251/.local/lib/python3.11/site-packages (from jinja2->torch>=1.4->timm==0.6.7->-r requirements.txt (line 1)) (3.0.2)

[notice] A new release of pip is available: 23.2.1 -> 25.0.1
[notice] To update, run: pip install --upgrade pip
| distributed init (rank 0): env://
[rank0]:[W316 15:30:19.201098359 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Task 1
Training Samples: 500
Validation Samples: 100
Replay Buffer Class Distribution: {2: 10, 1: 10}

Task 2
Training Samples: 270
Validation Samples: 50
Replay Buffer Class Distribution: {2: 7, 1: 7, 4: 6}

Task 3
Training Samples: 270
Validation Samples: 50
Replay Buffer Class Distribution: {2: 5, 1: 5, 4: 5, 0: 5}

Task 4
Training Samples: 270
Validation Samples: 50
Replay Buffer Class Distribution: {2: 4, 1: 4, 4: 4, 0: 4, 3: 4}

Creating original model: vit_base_patch16_224_dino
Creating model: vit_base_patch16_224_dino
Namespace(subparser_name='icmem_l2p', batch_size=16, epochs=5, model='vit_base_patch16_224_dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='local_datasets', dataset='Split-ICMem', baseline_file='high_mem_images.json', replay_size=20, shuffle='True', output_dir='icmem_output', device='cuda', seed=40, eval=False, num_workers=4, pin_mem=True, custom_weight_path=None, world_size=1, dist_url='env://', num_tasks=4, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=5)
number of params: 49925
Start training for 5 epochs
Task 1:
  Train Classes: [1, 2] (2 classes)
  Val Classes: [1, 2] (2 classes)
  Training Samples: 500
  Validation Samples: 100

Train: Epoch[ 1/10]  [ 0/32]  eta: 0:03:51  Lr: 0.001000  Loss: 0.7616  Acc@1: 43.7500 (43.7500)  Acc@5: 100.0000 (100.0000)  time: 7.2398  data: 2.0362  max mem: 2370
Train: Epoch[ 1/10]  [10/32]  eta: 0:00:18  Lr: 0.001000  Loss: 0.4736  Acc@1: 75.0000 (66.4773)  Acc@5: 100.0000 (100.0000)  time: 0.8454  data: 0.2437  max mem: 2370
Train: Epoch[ 1/10]  [20/32]  eta: 0:00:06  Lr: 0.001000  Loss: 0.2827  Acc@1: 87.5000 (78.5714)  Acc@5: 100.0000 (100.0000)  time: 0.2121  data: 0.0732  max mem: 2370
Train: Epoch[ 1/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: 0.0575  Acc@1: 93.7500 (84.2742)  Acc@5: 100.0000 (100.0000)  time: 0.2984  data: 0.1623  max mem: 2370
Train: Epoch[ 1/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: 0.0764  Acc@1: 93.7500 (84.4000)  Acc@5: 100.0000 (100.0000)  time: 0.2940  data: 0.1623  max mem: 2370
Train: Epoch[ 1/10] Total time: 0:00:15 (0.4795 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.0764  Acc@1: 93.7500 (84.4000)  Acc@5: 100.0000 (100.0000)
Train: Epoch[ 2/10]  [ 0/32]  eta: 0:00:09  Lr: 0.001000  Loss: 0.1668  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.3011  data: 0.1638  max mem: 2370
Train: Epoch[ 2/10]  [10/32]  eta: 0:00:03  Lr: 0.001000  Loss: 0.0770  Acc@1: 100.0000 (96.5909)  Acc@5: 100.0000 (100.0000)  time: 0.1510  data: 0.0150  max mem: 2370
Train: Epoch[ 2/10]  [20/32]  eta: 0:00:01  Lr: 0.001000  Loss: 0.0451  Acc@1: 100.0000 (96.7262)  Acc@5: 100.0000 (100.0000)  time: 0.1360  data: 0.0001  max mem: 2370
Train: Epoch[ 2/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: 0.0029  Acc@1: 100.0000 (95.9677)  Acc@5: 100.0000 (100.0000)  time: 0.1360  data: 0.0001  max mem: 2370
Train: Epoch[ 2/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: 0.0830  Acc@1: 100.0000 (96.0000)  Acc@5: 100.0000 (100.0000)  time: 0.1312  data: 0.0001  max mem: 2370
Train: Epoch[ 2/10] Total time: 0:00:04 (0.1393 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.0830  Acc@1: 100.0000 (96.0000)  Acc@5: 100.0000 (100.0000)
Train: Epoch[ 3/10]  [ 0/32]  eta: 0:00:09  Lr: 0.001000  Loss: 0.0837  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.2979  data: 0.1610  max mem: 2370
Train: Epoch[ 3/10]  [10/32]  eta: 0:00:03  Lr: 0.001000  Loss: -0.0036  Acc@1: 100.0000 (97.7273)  Acc@5: 100.0000 (100.0000)  time: 0.1509  data: 0.0148  max mem: 2370
Train: Epoch[ 3/10]  [20/32]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0183  Acc@1: 100.0000 (96.7262)  Acc@5: 100.0000 (100.0000)  time: 0.1362  data: 0.0001  max mem: 2370
Train: Epoch[ 3/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: 0.1120  Acc@1: 100.0000 (96.5726)  Acc@5: 100.0000 (100.0000)  time: 0.1360  data: 0.0001  max mem: 2370
Train: Epoch[ 3/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0342  Acc@1: 100.0000 (96.6000)  Acc@5: 100.0000 (100.0000)  time: 0.1312  data: 0.0001  max mem: 2370
Train: Epoch[ 3/10] Total time: 0:00:04 (0.1398 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0342  Acc@1: 100.0000 (96.6000)  Acc@5: 100.0000 (100.0000)
Train: Epoch[ 4/10]  [ 0/32]  eta: 0:00:09  Lr: 0.001000  Loss: -0.0238  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3119  data: 0.1750  max mem: 2370
Train: Epoch[ 4/10]  [10/32]  eta: 0:00:03  Lr: 0.001000  Loss: 0.0896  Acc@1: 100.0000 (96.5909)  Acc@5: 100.0000 (100.0000)  time: 0.1522  data: 0.0160  max mem: 2370
Train: Epoch[ 4/10]  [20/32]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0286  Acc@1: 100.0000 (96.7262)  Acc@5: 100.0000 (100.0000)  time: 0.1362  data: 0.0001  max mem: 2370
Train: Epoch[ 4/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0526  Acc@1: 100.0000 (96.9758)  Acc@5: 100.0000 (100.0000)  time: 0.1360  data: 0.0001  max mem: 2370
Train: Epoch[ 4/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: 0.0322  Acc@1: 100.0000 (97.0000)  Acc@5: 100.0000 (100.0000)  time: 0.1312  data: 0.0001  max mem: 2370
Train: Epoch[ 4/10] Total time: 0:00:04 (0.1398 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.0322  Acc@1: 100.0000 (97.0000)  Acc@5: 100.0000 (100.0000)
Train: Epoch[ 5/10]  [ 0/32]  eta: 0:00:09  Lr: 0.001000  Loss: 0.0623  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.2997  data: 0.1635  max mem: 2370
Train: Epoch[ 5/10]  [10/32]  eta: 0:00:03  Lr: 0.001000  Loss: 0.0530  Acc@1: 100.0000 (97.7273)  Acc@5: 100.0000 (100.0000)  time: 0.1509  data: 0.0150  max mem: 2370
Train: Epoch[ 5/10]  [20/32]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0228  Acc@1: 100.0000 (98.2143)  Acc@5: 100.0000 (100.0000)  time: 0.1360  data: 0.0001  max mem: 2370
Train: Epoch[ 5/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: 0.1079  Acc@1: 100.0000 (97.7823)  Acc@5: 100.0000 (100.0000)  time: 0.1359  data: 0.0001  max mem: 2370
Train: Epoch[ 5/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0551  Acc@1: 100.0000 (97.8000)  Acc@5: 100.0000 (100.0000)  time: 0.1311  data: 0.0001  max mem: 2370
Train: Epoch[ 5/10] Total time: 0:00:04 (0.1396 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0551  Acc@1: 100.0000 (97.8000)  Acc@5: 100.0000 (100.0000)
Train: Epoch[ 6/10]  [ 0/32]  eta: 0:00:10  Lr: 0.001000  Loss: -0.0084  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3296  data: 0.1929  max mem: 2370
Train: Epoch[ 6/10]  [10/32]  eta: 0:00:03  Lr: 0.001000  Loss: -0.0142  Acc@1: 100.0000 (98.8636)  Acc@5: 100.0000 (100.0000)  time: 0.1537  data: 0.0177  max mem: 2370
Train: Epoch[ 6/10]  [20/32]  eta: 0:00:01  Lr: 0.001000  Loss: 0.1166  Acc@1: 100.0000 (98.2143)  Acc@5: 100.0000 (100.0000)  time: 0.1360  data: 0.0002  max mem: 2370
Train: Epoch[ 6/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0610  Acc@1: 100.0000 (98.1855)  Acc@5: 100.0000 (100.0000)  time: 0.1359  data: 0.0001  max mem: 2370
Train: Epoch[ 6/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0504  Acc@1: 100.0000 (98.2000)  Acc@5: 100.0000 (100.0000)  time: 0.1311  data: 0.0001  max mem: 2370
Train: Epoch[ 6/10] Total time: 0:00:04 (0.1402 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0504  Acc@1: 100.0000 (98.2000)  Acc@5: 100.0000 (100.0000)
Train: Epoch[ 7/10]  [ 0/32]  eta: 0:00:09  Lr: 0.001000  Loss: 0.0232  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.2944  data: 0.1568  max mem: 2370
Train: Epoch[ 7/10]  [10/32]  eta: 0:00:03  Lr: 0.001000  Loss: -0.0588  Acc@1: 100.0000 (97.7273)  Acc@5: 100.0000 (100.0000)  time: 0.1505  data: 0.0144  max mem: 2370
Train: Epoch[ 7/10]  [20/32]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0690  Acc@1: 100.0000 (97.9167)  Acc@5: 100.0000 (100.0000)  time: 0.1361  data: 0.0002  max mem: 2370
Train: Epoch[ 7/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0781  Acc@1: 100.0000 (98.3871)  Acc@5: 100.0000 (100.0000)  time: 0.1359  data: 0.0001  max mem: 2370
Train: Epoch[ 7/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0890  Acc@1: 100.0000 (98.4000)  Acc@5: 100.0000 (100.0000)  time: 0.1311  data: 0.0001  max mem: 2370
Train: Epoch[ 7/10] Total time: 0:00:04 (0.1395 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0890  Acc@1: 100.0000 (98.4000)  Acc@5: 100.0000 (100.0000)
Train: Epoch[ 8/10]  [ 0/32]  eta: 0:00:09  Lr: 0.001000  Loss: 0.0182  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.2898  data: 0.1531  max mem: 2370
Train: Epoch[ 8/10]  [10/32]  eta: 0:00:03  Lr: 0.001000  Loss: 0.0093  Acc@1: 100.0000 (98.2955)  Acc@5: 100.0000 (100.0000)  time: 0.1500  data: 0.0140  max mem: 2370
Train: Epoch[ 8/10]  [20/32]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0651  Acc@1: 100.0000 (98.5119)  Acc@5: 100.0000 (100.0000)  time: 0.1359  data: 0.0001  max mem: 2370
Train: Epoch[ 8/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0593  Acc@1: 100.0000 (98.5887)  Acc@5: 100.0000 (100.0000)  time: 0.1358  data: 0.0001  max mem: 2370
Train: Epoch[ 8/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0786  Acc@1: 100.0000 (98.6000)  Acc@5: 100.0000 (100.0000)  time: 0.1310  data: 0.0001  max mem: 2370
Train: Epoch[ 8/10] Total time: 0:00:04 (0.1390 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0786  Acc@1: 100.0000 (98.6000)  Acc@5: 100.0000 (100.0000)
Train: Epoch[ 9/10]  [ 0/32]  eta: 0:00:09  Lr: 0.001000  Loss: -0.0884  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3015  data: 0.1642  max mem: 2370
Train: Epoch[ 9/10]  [10/32]  eta: 0:00:03  Lr: 0.001000  Loss: -0.0942  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.1511  data: 0.0151  max mem: 2370
Train: Epoch[ 9/10]  [20/32]  eta: 0:00:01  Lr: 0.001000  Loss: -0.1018  Acc@1: 100.0000 (99.1071)  Acc@5: 100.0000 (100.0000)  time: 0.1360  data: 0.0001  max mem: 2370
Train: Epoch[ 9/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0885  Acc@1: 100.0000 (99.3952)  Acc@5: 100.0000 (100.0000)  time: 0.1359  data: 0.0001  max mem: 2370
Train: Epoch[ 9/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0652  Acc@1: 100.0000 (99.4000)  Acc@5: 100.0000 (100.0000)  time: 0.1311  data: 0.0001  max mem: 2370
Train: Epoch[ 9/10] Total time: 0:00:04 (0.1394 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0652  Acc@1: 100.0000 (99.4000)  Acc@5: 100.0000 (100.0000)
Train: Epoch[10/10]  [ 0/32]  eta: 0:00:10  Lr: 0.001000  Loss: -0.0937  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3226  data: 0.1862  max mem: 2370
Train: Epoch[10/10]  [10/32]  eta: 0:00:03  Lr: 0.001000  Loss: -0.1045  Acc@1: 100.0000 (97.1591)  Acc@5: 100.0000 (100.0000)  time: 0.1531  data: 0.0171  max mem: 2370
Train: Epoch[10/10]  [20/32]  eta: 0:00:01  Lr: 0.001000  Loss: -0.1140  Acc@1: 100.0000 (97.3214)  Acc@5: 100.0000 (100.0000)  time: 0.1361  data: 0.0001  max mem: 2370
Train: Epoch[10/10]  [30/32]  eta: 0:00:00  Lr: 0.001000  Loss: -0.1015  Acc@1: 100.0000 (97.7823)  Acc@5: 100.0000 (100.0000)  time: 0.1359  data: 0.0001  max mem: 2370
Train: Epoch[10/10]  [31/32]  eta: 0:00:00  Lr: 0.001000  Loss: 0.0324  Acc@1: 100.0000 (97.8000)  Acc@5: 100.0000 (100.0000)  time: 0.1311  data: 0.0001  max mem: 2370
Train: Epoch[10/10] Total time: 0:00:04 (0.1400 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.0324  Acc@1: 100.0000 (97.8000)  Acc@5: 100.0000 (100.0000)
Standard Deviation of Similarities for task 1: 0.0630
Validation - Task 1:
  Classes: [1, 2] (2 classes)
  Samples: 100

Test: [Task 1]  [0/7]  eta: 0:00:18  Loss: 0.0103 (0.0103)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 2.6061  data: 2.4879  max mem: 2370
Test: [Task 1]  [6/7]  eta: 0:00:00  Loss: 0.0107 (0.0114)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.6092  data: 0.5255  max mem: 2370
Test: [Task 1] Total time: 0:00:04 (0.6145 s / it)
* Acc@1 100.000 Acc@5 100.000 loss 0.011
[Average accuracy till task1]	Acc@1: 100.0000	Acc@5: 100.0000	Loss: 0.0114
Task 2:
  Train Classes: [1, 2, 4] (3 classes)
  Val Classes: [4] (1 classes)
  Training Samples: 270
  Validation Samples: 50

Train: Epoch[1/5]  [ 0/17]  eta: 0:00:23  Lr: 0.001000  Loss: 2.2162  Acc@1: 0.0000 (0.0000)  Acc@5: 100.0000 (100.0000)  time: 1.3883  data: 1.2167  max mem: 2370
Train: Epoch[1/5]  [10/17]  eta: 0:00:03  Lr: 0.001000  Loss: 0.1311  Acc@1: 93.7500 (76.7045)  Acc@5: 100.0000 (100.0000)  time: 0.4403  data: 0.3000  max mem: 2370
Train: Epoch[1/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: 0.1021  Acc@1: 93.7500 (82.9630)  Acc@5: 100.0000 (100.0000)  time: 0.4145  data: 0.2756  max mem: 2370
Train: Epoch[1/5] Total time: 0:00:07 (0.4174 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.1021  Acc@1: 93.7500 (82.9630)  Acc@5: 100.0000 (100.0000)
Train: Epoch[2/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: 0.0600  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.3063  data: 0.1694  max mem: 2381
Train: Epoch[2/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0310  Acc@1: 100.0000 (98.2955)  Acc@5: 100.0000 (100.0000)  time: 0.1516  data: 0.0155  max mem: 2381
Train: Epoch[2/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0525  Acc@1: 100.0000 (98.1481)  Acc@5: 100.0000 (100.0000)  time: 0.1451  data: 0.0101  max mem: 2381
Train: Epoch[2/5] Total time: 0:00:02 (0.1474 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0525  Acc@1: 100.0000 (98.1481)  Acc@5: 100.0000 (100.0000)
Train: Epoch[3/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: -0.0314  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3108  data: 0.1737  max mem: 2381
Train: Epoch[3/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0485  Acc@1: 100.0000 (95.4545)  Acc@5: 100.0000 (100.0000)  time: 0.1524  data: 0.0160  max mem: 2381
Train: Epoch[3/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0493  Acc@1: 100.0000 (96.6667)  Acc@5: 100.0000 (100.0000)  time: 0.1458  data: 0.0104  max mem: 2381
Train: Epoch[3/5] Total time: 0:00:02 (0.1481 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0493  Acc@1: 100.0000 (96.6667)  Acc@5: 100.0000 (100.0000)
Train: Epoch[4/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: 0.0923  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.3068  data: 0.1693  max mem: 2381
Train: Epoch[4/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: 0.1932  Acc@1: 100.0000 (97.1591)  Acc@5: 100.0000 (100.0000)  time: 0.1516  data: 0.0155  max mem: 2381
Train: Epoch[4/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0540  Acc@1: 100.0000 (97.0370)  Acc@5: 100.0000 (100.0000)  time: 0.1451  data: 0.0101  max mem: 2381
Train: Epoch[4/5] Total time: 0:00:02 (0.1475 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0540  Acc@1: 100.0000 (97.0370)  Acc@5: 100.0000 (100.0000)
Train: Epoch[5/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: -0.0133  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.3025  data: 0.1647  max mem: 2381
Train: Epoch[5/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: 0.4956  Acc@1: 100.0000 (96.0227)  Acc@5: 100.0000 (100.0000)  time: 0.1512  data: 0.0151  max mem: 2381
Train: Epoch[5/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0749  Acc@1: 100.0000 (96.2963)  Acc@5: 100.0000 (100.0000)  time: 0.1449  data: 0.0098  max mem: 2381
Train: Epoch[5/5] Total time: 0:00:02 (0.1472 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0749  Acc@1: 100.0000 (96.2963)  Acc@5: 100.0000 (100.0000)
Standard Deviation of Similarities for task 2: 0.0649
Validation - Task 1:
  Classes: [1, 2] (2 classes)
  Samples: 100

Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Test: [Task 1]  [0/7]  eta: 0:00:02  Loss: 0.3168 (0.3168)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3090  data: 0.2212  max mem: 2381
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Guess: 4 | Actual: 1
Test: [Task 1]  [6/7]  eta: 0:00:00  Loss: 0.3037 (0.2851)  Acc@1: 87.5000 (86.0000)  Acc@5: 100.0000 (100.0000)  time: 0.1097  data: 0.0317  max mem: 2381
Test: [Task 1] Total time: 0:00:00 (0.1150 s / it)
* Acc@1 86.000 Acc@5 100.000 loss 0.285
Validation - Task 2:
  Classes: [4] (1 classes)
  Samples: 50

Test: [Task 2]  [0/4]  eta: 0:00:08  Loss: 0.0528 (0.0528)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 2.0339  data: 1.9244  max mem: 2381
Test: [Task 2]  [3/4]  eta: 0:00:00  Loss: 0.0350 (0.0477)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5955  data: 0.5181  max mem: 2381
Test: [Task 2] Total time: 0:00:02 (0.6052 s / it)
* Acc@1 100.000 Acc@5 100.000 loss 0.048
[Average accuracy till task2]	Acc@1: 93.0000	Acc@5: 100.0000	Loss: 0.1664	Forgetting: 14.0000	Backward: -14.0000
Task 3:
  Train Classes: [0, 1, 2, 4] (4 classes)
  Val Classes: [0] (1 classes)
  Training Samples: 270
  Validation Samples: 50

Train: Epoch[1/5]  [ 0/17]  eta: 0:00:31  Lr: 0.001000  Loss: 1.9380  Acc@1: 6.2500 (6.2500)  Acc@5: 100.0000 (100.0000)  time: 1.8717  data: 1.7002  max mem: 2381
Train: Epoch[1/5]  [10/17]  eta: 0:00:03  Lr: 0.001000  Loss: 0.3923  Acc@1: 93.7500 (78.4091)  Acc@5: 100.0000 (100.0000)  time: 0.4382  data: 0.2971  max mem: 2381
Train: Epoch[1/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: 0.6162  Acc@1: 93.7500 (82.9630)  Acc@5: 100.0000 (100.0000)  time: 0.4940  data: 0.3557  max mem: 2381
Train: Epoch[1/5] Total time: 0:00:08 (0.4963 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.6162  Acc@1: 93.7500 (82.9630)  Acc@5: 100.0000 (100.0000)
Train: Epoch[2/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: -0.0503  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3047  data: 0.1669  max mem: 2381
Train: Epoch[2/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0580  Acc@1: 100.0000 (96.5909)  Acc@5: 100.0000 (100.0000)  time: 0.1515  data: 0.0153  max mem: 2381
Train: Epoch[2/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: 0.0402  Acc@1: 93.7500 (95.5556)  Acc@5: 100.0000 (100.0000)  time: 0.1451  data: 0.0099  max mem: 2381
Train: Epoch[2/5] Total time: 0:00:02 (0.1474 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.0402  Acc@1: 93.7500 (95.5556)  Acc@5: 100.0000 (100.0000)
Train: Epoch[3/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: 0.1180  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3181  data: 0.1810  max mem: 2381
Train: Epoch[3/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0675  Acc@1: 100.0000 (96.5909)  Acc@5: 100.0000 (100.0000)  time: 0.1527  data: 0.0166  max mem: 2381
Train: Epoch[3/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: 0.4115  Acc@1: 100.0000 (96.2963)  Acc@5: 100.0000 (100.0000)  time: 0.1459  data: 0.0107  max mem: 2381
Train: Epoch[3/5] Total time: 0:00:02 (0.1479 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.4115  Acc@1: 100.0000 (96.2963)  Acc@5: 100.0000 (100.0000)
Train: Epoch[4/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: -0.0716  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3166  data: 0.1793  max mem: 2381
Train: Epoch[4/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: 0.0244  Acc@1: 100.0000 (96.5909)  Acc@5: 100.0000 (100.0000)  time: 0.1524  data: 0.0164  max mem: 2381
Train: Epoch[4/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: 0.0818  Acc@1: 100.0000 (97.0370)  Acc@5: 100.0000 (100.0000)  time: 0.1455  data: 0.0106  max mem: 2381
Train: Epoch[4/5] Total time: 0:00:02 (0.1485 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.0818  Acc@1: 100.0000 (97.0370)  Acc@5: 100.0000 (100.0000)
Train: Epoch[5/5]  [ 0/17]  eta: 0:00:06  Lr: 0.001000  Loss: -0.0098  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.3950  data: 0.2581  max mem: 2381
Train: Epoch[5/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: 0.0335  Acc@1: 93.7500 (94.8864)  Acc@5: 100.0000 (100.0000)  time: 0.1596  data: 0.0236  max mem: 2381
Train: Epoch[5/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0624  Acc@1: 100.0000 (96.6667)  Acc@5: 100.0000 (100.0000)  time: 0.1502  data: 0.0153  max mem: 2381
Train: Epoch[5/5] Total time: 0:00:02 (0.1525 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0624  Acc@1: 100.0000 (96.6667)  Acc@5: 100.0000 (100.0000)
Standard Deviation of Similarities for task 3: 0.0720
Validation - Task 1:
  Classes: [1, 2] (2 classes)
  Samples: 100

Test: [Task 1]  [0/7]  eta: 0:00:02  Loss: 0.0883 (0.0883)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3055  data: 0.2179  max mem: 2381
Guess: 0 | Actual: 1
Guess: 0 | Actual: 1
Guess: 0 | Actual: 1
Test: [Task 1]  [6/7]  eta: 0:00:00  Loss: 0.1014 (0.1227)  Acc@1: 100.0000 (97.0000)  Acc@5: 100.0000 (100.0000)  time: 0.1092  data: 0.0312  max mem: 2381
Test: [Task 1] Total time: 0:00:00 (0.1164 s / it)
* Acc@1 97.000 Acc@5 100.000 loss 0.123
Validation - Task 2:
  Classes: [4] (1 classes)
  Samples: 50

Guess: 0 | Actual: 4
Guess: 0 | Actual: 4
Guess: 0 | Actual: 4
Guess: 0 | Actual: 4
Test: [Task 2]  [0/4]  eta: 0:00:01  Loss: 0.5802 (0.5802)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.2810  data: 0.1937  max mem: 2381
Guess: 0 | Actual: 4
Guess: 0 | Actual: 4
Guess: 0 | Actual: 4
Guess: 0 | Actual: 4
Guess: 0 | Actual: 4
Guess: 0 | Actual: 4
Test: [Task 2]  [3/4]  eta: 0:00:00  Loss: 0.5233 (0.5148)  Acc@1: 81.2500 (80.0000)  Acc@5: 100.0000 (100.0000)  time: 0.1177  data: 0.0485  max mem: 2381
Test: [Task 2] Total time: 0:00:00 (0.1313 s / it)
* Acc@1 80.000 Acc@5 100.000 loss 0.515
Validation - Task 3:
  Classes: [0] (1 classes)
  Samples: 50

Test: [Task 3]  [0/4]  eta: 0:00:07  Loss: 0.0046 (0.0046)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 1.7607  data: 1.6514  max mem: 2381
Test: [Task 3]  [3/4]  eta: 0:00:00  Loss: 0.0046 (0.0085)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.4890  data: 0.4129  max mem: 2381
Test: [Task 3] Total time: 0:00:02 (0.5010 s / it)
* Acc@1 100.000 Acc@5 100.000 loss 0.009
[Average accuracy till task3]	Acc@1: 92.3333	Acc@5: 100.0000	Loss: 0.2154	Forgetting: 11.5000	Backward: -11.5000
Task 4:
  Train Classes: [0, 1, 2, 3, 4] (5 classes)
  Val Classes: [3] (1 classes)
  Training Samples: 270
  Validation Samples: 50

Train: Epoch[1/5]  [ 0/17]  eta: 0:00:31  Lr: 0.001000  Loss: 3.2536  Acc@1: 0.0000 (0.0000)  Acc@5: 100.0000 (100.0000)  time: 1.8474  data: 1.6859  max mem: 2381
Train: Epoch[1/5]  [10/17]  eta: 0:00:04  Lr: 0.001000  Loss: 0.6778  Acc@1: 87.5000 (68.1818)  Acc@5: 100.0000 (100.0000)  time: 0.5956  data: 0.4579  max mem: 2381
Train: Epoch[1/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: 0.4758  Acc@1: 92.8571 (75.9259)  Acc@5: 100.0000 (100.0000)  time: 0.4553  data: 0.3195  max mem: 2381
Train: Epoch[1/5] Total time: 0:00:07 (0.4582 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.4758  Acc@1: 92.8571 (75.9259)  Acc@5: 100.0000 (100.0000)
Train: Epoch[2/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: 0.6632  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3299  data: 0.1931  max mem: 2381
Train: Epoch[2/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: 0.0025  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.1532  data: 0.0177  max mem: 2381
Train: Epoch[2/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: 0.0046  Acc@1: 93.7500 (94.8148)  Acc@5: 100.0000 (100.0000)  time: 0.1459  data: 0.0115  max mem: 2381
Train: Epoch[2/5] Total time: 0:00:02 (0.1491 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.0046  Acc@1: 93.7500 (94.8148)  Acc@5: 100.0000 (100.0000)
Train: Epoch[3/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: -0.0536  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3046  data: 0.1678  max mem: 2381
Train: Epoch[3/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0018  Acc@1: 93.7500 (94.8864)  Acc@5: 100.0000 (100.0000)  time: 0.1508  data: 0.0154  max mem: 2381
Train: Epoch[3/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: 0.3367  Acc@1: 100.0000 (95.9259)  Acc@5: 100.0000 (100.0000)  time: 0.1444  data: 0.0100  max mem: 2381
Train: Epoch[3/5] Total time: 0:00:02 (0.1473 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.3367  Acc@1: 100.0000 (95.9259)  Acc@5: 100.0000 (100.0000)
Train: Epoch[4/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: -0.0711  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3192  data: 0.1815  max mem: 2381
Train: Epoch[4/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0242  Acc@1: 93.7500 (96.5909)  Acc@5: 100.0000 (100.0000)  time: 0.1521  data: 0.0166  max mem: 2381
Train: Epoch[4/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: 0.1169  Acc@1: 93.7500 (96.2963)  Acc@5: 100.0000 (100.0000)  time: 0.1452  data: 0.0108  max mem: 2381
Train: Epoch[4/5] Total time: 0:00:02 (0.1484 s / it)
Averaged stats: Lr: 0.001000  Loss: 0.1169  Acc@1: 93.7500 (96.2963)  Acc@5: 100.0000 (100.0000)
Train: Epoch[5/5]  [ 0/17]  eta: 0:00:05  Lr: 0.001000  Loss: -0.0152  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3329  data: 0.1965  max mem: 2381
Train: Epoch[5/5]  [10/17]  eta: 0:00:01  Lr: 0.001000  Loss: -0.0108  Acc@1: 100.0000 (99.4318)  Acc@5: 100.0000 (100.0000)  time: 0.1536  data: 0.0180  max mem: 2381
Train: Epoch[5/5]  [16/17]  eta: 0:00:00  Lr: 0.001000  Loss: -0.0895  Acc@1: 100.0000 (98.8889)  Acc@5: 100.0000 (100.0000)  time: 0.1462  data: 0.0117  max mem: 2381
Train: Epoch[5/5] Total time: 0:00:02 (0.1493 s / it)
Averaged stats: Lr: 0.001000  Loss: -0.0895  Acc@1: 100.0000 (98.8889)  Acc@5: 100.0000 (100.0000)
Standard Deviation of Similarities for task 4: 0.0769
Validation - Task 1:
  Classes: [1, 2] (2 classes)
  Samples: 100

Guess: 3 | Actual: 1
Guess: 3 | Actual: 2
Test: [Task 1]  [0/7]  eta: 0:00:02  Loss: 0.3020 (0.3020)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3324  data: 0.2444  max mem: 2381
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Guess: 3 | Actual: 1
Test: [Task 1]  [6/7]  eta: 0:00:00  Loss: 0.3020 (0.3214)  Acc@1: 87.5000 (87.0000)  Acc@5: 100.0000 (100.0000)  time: 0.1131  data: 0.0350  max mem: 2381
Test: [Task 1] Total time: 0:00:00 (0.1208 s / it)
* Acc@1 87.000 Acc@5 100.000 loss 0.321
Validation - Task 2:
  Classes: [4] (1 classes)
  Samples: 50

Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Test: [Task 2]  [0/4]  eta: 0:00:01  Loss: 1.3585 (1.3585)  Acc@1: 43.7500 (43.7500)  Acc@5: 100.0000 (100.0000)  time: 0.2910  data: 0.2032  max mem: 2381
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Guess: 3 | Actual: 4
Test: [Task 2]  [3/4]  eta: 0:00:00  Loss: 1.0078 (1.4032)  Acc@1: 43.7500 (46.0000)  Acc@5: 100.0000 (100.0000)  time: 0.1204  data: 0.0509  max mem: 2381
Test: [Task 2] Total time: 0:00:00 (0.1327 s / it)
* Acc@1 46.000 Acc@5 100.000 loss 1.403
Validation - Task 3:
  Classes: [0] (1 classes)
  Samples: 50

Guess: 3 | Actual: 0
Guess: 3 | Actual: 0
Guess: 3 | Actual: 0
Test: [Task 3]  [0/4]  eta: 0:00:01  Loss: 0.2194 (0.2194)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.2901  data: 0.2030  max mem: 2381
Guess: 3 | Actual: 0
Guess: 3 | Actual: 0
Guess: 1 | Actual: 0
Test: [Task 3]  [3/4]  eta: 0:00:00  Loss: 0.2164 (0.1861)  Acc@1: 87.5000 (88.0000)  Acc@5: 100.0000 (100.0000)  time: 0.1198  data: 0.0508  max mem: 2381
Test: [Task 3] Total time: 0:00:00 (0.1328 s / it)
* Acc@1 88.000 Acc@5 100.000 loss 0.186
Validation - Task 4:
  Classes: [3] (1 classes)
  Samples: 50

Test: [Task 4]  [0/4]  eta: 0:00:07  Loss: 0.0038 (0.0038)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 1.8940  data: 1.7844  max mem: 2381
Test: [Task 4]  [3/4]  eta: 0:00:00  Loss: 0.0038 (0.0038)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5250  data: 0.4503  max mem: 2381
Test: [Task 4] Total time: 0:00:02 (0.5334 s / it)
* Acc@1 100.000 Acc@5 100.000 loss 0.004
[Average accuracy till task4]	Acc@1: 80.2500	Acc@5: 100.0000	Loss: 0.4786	Forgetting: 26.3333	Backward: -26.3333
Total training time: 0:02:08
[rank0]:[W316 15:32:48.891769856 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			Resource Usage on 2025-03-16 15:32:51.733839:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	JobId: 9742479.pbs101
	Project: personal-e1506251
	Exit Status: 0
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	NCPUs: Requested(16), Used(16)
	CPU Time Used: 00:03:12
	Memory: Requested(110gb), Used(9878000kb)
	Vmem Used: 4056788kb
	Walltime: Requested(02:00:00), Used(00:09:54)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	Execution Nodes Used: (x1000c0s0b0n1:ncpus=16:mem=115343360kb:ngpus=1)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	GPU Duration: 9.92mins
	GPU Power Consumed: 93.26W
	GPU Max GPU Memory Used: 3.62GB
	Memory Throughput Rate (Average): x1000c0s0b0n1:(gpu1:2%)
	Memory Throughput Rate (Max): x1000c0s0b0n1:(gpu1:19%)
	Memory Throughput Rate (Min): x1000c0s0b0n1:(gpu1:0%)
	GPU SM Utilization (Average): x1000c0s0b0n1:(gpu1:16%)
	GPU SM Utilization (Max): x1000c0s0b0n1:(gpu1:99%)
	GPU SM Utilization (Min): x1000c0s0b0n1:(gpu1:0%)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Warning: None
GPU application profile: Low
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

